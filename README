README file for Programming Assignment 2 (C++ edition)
=====================================================

Your directory should now contain the following files:

 Makefile        -> [course dir]/src/PA2/Makefile
 README
 cool.flex
 test.cl
 lextest.cc      -> [course dir]/src/PA2/lextest.cc
 mycoolc         -> [course dir]/src/PA2/mycoolc
 stringtab.cc    -> [course dir]/src/PA2/stringtab.cc
 utilities.cc    -> [course dir]/src/PA2/utilities.cc
 handle_flags.cc -> [course dir]/src/PA2/handle_flags.cc
 *.d             dependency files
 *.*             other generated files

The include (.h) files for this assignment can be found in 
[course dir]/include/PA2

	The Makefile contains targets for compiling and running your
	program. DO NOT MODIFY.

	The README contains this info. Part of the assignment is to fill
	the README with the write-up for your project. You should
	explain design decisions, explain why your code is correct, and
	why your test cases are adequate. It is part of the assignment
	to clearly and concisely explain things in text as well as to
	comment your code. Just edit this file.

	cool.flex is a skeleton file for the specification of the
	lexical analyzer. You should complete it with your regular
	expressions, patterns and actions. Information on how to do this
	is in the flex manual, which is part of your reader.

	test.cl is a COOL program that you can test the lexical
	analyzer on. It contains some errors, so it won't compile with
	coolc. However, test.cl does not exercise all lexical
	constructs of COOL and part of your assignment is to rewrite
	test.cl with a complete set of tests for your lexical analyzer.

	cool-parse.h contains definitions that are used by almost all parts
	of the compiler. DO NOT MODIFY.

	stringtab.{cc|h} and stringtab_functions.h contains functions
        to manipulate the string tables.  DO NOT MODIFY.

	utilities.{cc|h} contains functions used by the main() part of
	the lextest program. You may want to use the strdup() function
	defined in here. Remember that you should not print anything
	from inside cool.flex! DO NOT MODIFY.

	lextest.cc contains the main function which will call your
	lexer and print out the tokens that it returns.  DO NOT MODIFY.

	mycoolc is a shell script that glues together the phases of the
	compiler using Unix pipes instead of statically linking code.  
	While inefficient, this architecture makes it easy to mix and match
	the components you write with those of the course compiler.
	DO NOT MODIFY.	

        cool-lexer.cc is the scanner generated by flex from cool.flex.
        DO NOT MODIFY IT, as your changes will be overritten the next
        time you run flex.

 	The *.d files are automatically generated Makefiles that capture
 	dependencies between source and header files in this directory.
 	These files are updated automatically by Makefile; see the gmake
 	documentation for a detailed explanation.

Instructions
----------------------------------------------

	To compile your lextest program type:

	% gmake lexer

	Run your lexer by putting your test input in a file 'foo.cl' and
	run the lextest program:

	% ./lexer foo.cl

	To run your lexer on the file test.cl type:

	% gmake dotest

	If you think your lexical analyzer is correct and behaves like
	the one we wrote, you can actually try 'mycoolc' and see whether
	it runs and produces correct code for any examples.
	If your lexical analyzer behaves in an
	unexpected manner, you may get errors anywhere, i.e. during
	parsing, during semantic analysis, during code generation or
	only when you run the produced code on spim. So beware.

	To turnin your work type:

	% gmake submit-clean

	And run the "submit" program following the instructions on the
	course web page.
	
	Running "submit" will collect the files cool.flex, test.cl,
	README, and test.output. Don't forget to edit the README file to
	include your write-up, and to write your own test cases in
	test.cl.

 	You may turn in the assignment as many times as you like.
	However, only the last version will be retained for
	grading.

	If you change architectures you must issue

	% gmake clean

	when you switch from one type of machine to the other.
	If at some point you get weird errors from the linker,	
	you probably forgot this step.

	GOOD LUCK!

---8<------8<------8<------8<---cut here---8<------8<------8<------8<---

Lexical Analyzer

Here are the details of the implementation strategy and challenges faced while developing the code for the cool.flex file. This implementation was carried out using Flex as part of PA2 to build the scanner for the Cool language. The choice of Flex was primarily due to its extensive documentation, which facilitated both analysis and reference throughout the development process.

The scanner operates by reading the teste.cl file line by line, processing each character to determine whether it matches any predefined rules in the code. If a match is found, the scanner returns the corresponding token and continues the analysis. Otherwise, it generates an error message or returns an invalid value, ensuring that any unrecognized input is handled properly.

To enable the scanner’s functionality, the code begins by including the necessary libraries. These libraries were already present in the original file and remained unchanged. However, significant modifications and additions were made to enhance the scanner’s capabilities. One of the key improvements was the explicit handling of token returns, ensuring that each identified token—such as operators and literals—was correctly recognized. Additionally, specific adjustments were made to distinguish keywords like IF, ELSE, CLASS, and true from generic identifiers, preventing misinterpretation.

Another crucial enhancement involved defining tokens for multi-character expressions, such as DARROW (=>), ASSIGN (<-), and LE (<=). Furthermore, rules were established to identify integer constants (INT_CONST), type names (TYPEID), and general identifiers (ID). Initially, an issue arose in the order of identifier rules: TYPEID was placed after ID, which caused certain identifiers that should have been classified as TYPEID to be recognized incorrectly as ID. This was resolved by reversing their order, ensuring that TYPEID was processed first. The corrected implementation was structured as follows:

{TYPEID} { cool_yylval.symbol = stringtable.add_string(yytext); return TYPEID; }  
{ID} { cool_yylval.symbol = stringtable.add_string(yytext); return OBJECTID; }

Special handling was also required for integer constants to ensure that only valid numeric sequences were recognized. A specific rule was implemented to associate numerical values with the symbol table (inttable), allowing the compiler to store and reference them appropriately:

{INT_CONST} { cool_yylval.symbol = inttable.add_string(yytext); return INT_CONST; }  

Another significant challenge was implementing support for nested multi-line comments. To achieve this, a counter was used to track the level of nesting. If a comment was opened but not properly closed, the scanner would generate a lexical error, indicating the line number where the issue occurred. The mechanism carefully scanned each character to correctly detect comment delimiters:

"(*" {  
    int nested = 1;  
    char c;  

    while (nested > 0) {  
        c = yyinput();  

        if (c == EOF) {  
            fprintf(stderr, "Lexical error on line %d: unclosed comment\n", curr_lineno);  
            return ERROR;  
        } else if (c == '\n') {  
            curr_lineno++;  
        } else if (c == '(') {  
            if ((c = yyinput()) == '*') nested++;  
        } else if (c == '*') {  
            if ((c = yyinput()) == ')') nested--;  
        }  
    }  
}  

Throughout development, several challenges emerged and were systematically resolved. One of the primary issues involved multi-line reading. Initially, the scanner was only processing a single line from the teste.cl file. After debugging, it was discovered that a crucial line of the original code had been deleted, causing this unintended behavior. Restoring the missing line corrected the issue, allowing the scanner to process the entire file as expected.

Another difficulty was handling special characters. Establishing a universal rule for all special characters proved challenging, as they each required distinct recognition criteria. The solution was to explicitly define rules for each character, ensuring that all were correctly processed by the scanner.

With these refinements, the scanner was successfully implemented, offering robust token recognition and error handling. The adjustments made not only improved accuracy but also ensured that the scanner adhered to the language specifications, providing a solid foundation for further development.
